{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30584\n"
     ]
    }
   ],
   "source": [
    "with open(\"..\\data\\szoveg.txt\", \"r\", encoding=\"utf8\") as f:     #megnyitjuk a beolvasnadó fájlt, majd kisbetűssé alakítjuk azt, végül kiírjuk a hosszát\n",
    "    text = f.read().lower()\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello, everyone! this is the longest text ever! i was inspired by the various other \"longest texts ever\" on the internet, and i wanted to make my own. so here it is! this is going to be a world record! this is actually my third attempt at doing this. the first time, i didn\\'t save it. the second time, the neocities editor crashed. now i\\'m writing this in notepad, then copying it into the neocities editor instead of typing it directly in the neocities editor to avoid crashing. it sucks that my past two attempts are gone now. those actually got pretty long. not the longest, but still pretty long. i hope this one won\\'t get lost somehow. anyways, let\\'s talk about waffles! i like waffles. waffles are cool. waffles is a funny word. there\\'s a teen titans go episode called \"waffles\" where the word \"waffles\" is said a hundred-something times. it\\'s pretty annoying. there\\'s also a teen titans go episode about pig latin. don\\'t know what pig latin is? it\\'s a language where you take all the consonants before the first vowel, move them to the end, and add \\'-ay\\' to the end. if the word begins with a vowel, you just add \\'-way\\' to the end. for example, \"waffles\" becomes \"afflesway\". i\\'ve been speaking pig latin fluently since the fourth grade, so it surprised me when i saw the episode for the first time. i speak pig latin with my sister sometimes. it\\'s pretty fun. i like speaking it in public so that everyone around us gets confused. that\\'s never actually happened before, but if it ever does, \\'twill be pretty funny. by the way, \"\\'twill\" is a word i invented recently, and it\\'s a contraction of \"it will\". i really hope it gains popularity in the near future, because \"\\'twill\" is way more fun than saying \"it\\'ll\". \"it\\'ll\" is too boring. nobody likes boring. this is nowhere near being the longest text ever, but eventually it will be! i might still be writing this a decade later, who knows? but right now, it\\'s not very long. but i\\'ll just keep writing until it is the longest! have you ever heard the song \"dau dau\" by awesome scampis? it\\'s an amazing song. look it up on youtube! i play that song all the time around my sister! it drives her crazy, and i love it. another way i like driving my sister crazy is by speaking my own made up language to her. she hates the languages i make! the only language that we both speak besides english is pig latin. i think you already knew that. whatever. i think i\\'m gonna go for now. bye! hi, i\\'m back now. i\\'m gonna contribute more to this soon-to-be giant wall of text. i just realised i have a giant stuffed frog on my bed. i forgot his name. i\\'m pretty sure it was something stupid though. i think it was \"frog\" in morse code or something. morse code is cool. i know a bit of it, but i\\'m not very good at it. i\\'m also not very good at french. i barely know anything in french, and my pronunciation probably sucks. but i\\'m learning it, at least. i\\'m also learning esperanto. it\\'s this language that was made up by some guy a long time ago to be the \"universal language\". a lot of people speak it. i am such a language nerd. half of this text is probably gonna be about languages. but hey, as long as it\\'s long! ha, get it? as long as it\\'s long? i\\'m so funny, right? no, i\\'m not. i should probably get some sleep. goodnight! hello, i\\'m back again. i basically have only two interests nowadays: languages and furries. what? oh, sorry, i thought you knew i was a furry. haha, oops. anyway, yeah, i\\'m a furry, but since i\\'m a young furry, i can\\'t really do as much as i would like to do in the fandom. when i\\'m older, i would like to have a fursuit, go to furry conventions, all that stuff. but for now i can only dream of that. sorry you had to deal with me talking about furries, but i\\'m honestly very desperate for this to be the longest text ever. last night i was watching nothing but fursuit unboxings. i think i need help. this one time, me and my mom were going to go to a furry christmas party, but we didn\\'t end up going because of the fact that there was alcohol on the premises, and that she didn\\'t wanna have to be a mom dragging her son through a crowd of furries. both of those reasons were understandable. okay, hopefully i won\\'t have to talk about furries anymore. i don\\'t care if you\\'re a furry reading this right now, i just don\\'t wanna have to torture everyone else. i will no longer say the f word throughout the rest of this entire text. of course, by the f word, i mean the one that i just used six times, not the one that you\\'re probably thinking of which i have not used throughout this entire text. i just realised that next year will be 2020. that\\'s crazy! it just feels so futuristic! it\\'s also crazy that the 2010s decade is almost over. that decade brought be a lot of memories. in fact, it brought be almost all of my memories. it\\'ll be sad to see it go. i\\'m gonna work on a series of video lessons for toki pona. i\\'ll expain what toki pona is after i come back. bye! i\\'m back now, and i decided not to do it on toki pona, since many other people have done toki pona video lessons already. i decided to do it on viesa, my english code. now, i shall explain what toki pona is. toki pona is a minimalist constructed language that has only ~120 words! that means you can learn it very quickly. i reccomend you learn it! it\\'s pretty fun and easy! anyway, yeah, i might finish my video about viesa later. but for now, i\\'m gonna add more to this giant wall of text, because i want it to be the longest! it would be pretty cool to have a world record for the longest text ever. not sure how famous i\\'ll get from it, but it\\'ll be cool nonetheless. nonetheless. that\\'s an interesting word. it\\'s a combination of three entire words. that\\'s pretty neat. also, remember when i said that i said the f word six times throughout this text? i actually messed up there. i actually said it ten times (including the plural form). i\\'m such a liar! i struggled to spell the word \"liar\" there. i tried spelling it \"lyer\", then \"lier\". then i remembered that it\\'s \"liar\". at least i\\'m better at spelling than my sister. she\\'s younger than me, so i guess it\\'s understandable. \"understandable\" is a pretty long word. hey, i wonder what the most common word i\\'ve used so far in this text is. i checked, and appearantly it\\'s \"i\", with 59 uses! the word \"i\" makes up 5% of the words this text! i would\\'ve thought \"the\" would be the most common, but \"the\" is only the second most used word, with 43 uses. \"it\" is the third most common, followed by \"a\" and \"to\". congrats to those five words! if you\\'re wondering what the least common word is, well, it\\'s actually a tie between a bunch of words that are only used once, and i don\\'t wanna have to list them all here. remember when i talked about waffles near the beginning of this text? well, i just put some waffles in the toaster, and i got reminded of the very beginnings of this longest text ever. okay, that was literally yesterday, but i don\\'t care. you can\\'t see me right now, but i\\'m typing with my nose! okay, i was not able to type the exclamation point with just my nose. i had to use my finger. but still, i typed all of that sentence with my nose! i\\'m not typing with my nose right now, because it takes too long, and i wanna get this text as long as possible quickly. i\\'m gonna take a break for now! bye! hi, i\\'m back again. my sister is beside me, watching me write in this endless wall of text. my sister has a new thing where she just says the word \"poop\" nonstop. i don\\'t really like it. she also eats her own boogers. i\\'m not joking. she\\'s gross like that. also, remember when i said i put waffles in the toaster? well, i forgot about those and i only ate them just now. now my sister is just saying random numbers. now she\\'s saying that they\\'re not random, they\\'re the numbers being displayed on the microwave. still, i don\\'t know why she\\'s doing that. now she\\'s making annoying clicking noises. now she\\'s saying that she\\'s gonna watch friends on three different devices. why!?!?! hi its me his sister. i\\'d like to say that all of that is not true. max wants to make his own video but i wont let him because i need my phone for my alarm.poop poop poop poop lol im funny. kjnbhhisdnhidfhdfhjsdjksdnjhdfhdfghdfghdfbhdfbcbhnidjsduhchyduhyduhdhcduhduhdcdhcdhjdnjdnhjsdjxnj hey, i\\'m back. sorry about my sister. i had to seize control of the lte from her because she was doing keymash. keymash is just effortless. she just went back to school. she comes home from school for her lunch break. i think i\\'m gonna go again. bye! hello, i\\'m back. let\\'s compare lte\\'s. this one is only 8593 characters long so far. kenneth iman\\'s lte is 21425 characters long. the flaming-chicken lte (the original) is a whopping 203941 characters long! i think i\\'ll be able to surpass kenneth iman\\'s not long from now. but my goal is to surpass the flaming-chicken lte. actually, i just figured out that there\\'s an lte longer than the flaming-chicken lte. it\\'s hermnerps lte, which is only slightly longer than the flaming-chicken lte, at 230634 characters. my goal is to surpass that. then i\\'ll be the world record holder, i think. but i\\'ll still be writing this even after i achieve the world record, of course. one time, i printed an entire copy of the bee movie script for no reason.it\\'ll feel nice to be way ahead the record. my sister\\'s alarm clock has been going off for half an hour and i haven\\'t turned it off. why? because lazyness! actually, i really should turn it off now. there, i turned it off. first when i tried to turn it off, it started playing the radio. then i tried again, and it turned off completely. then i hurt myself on the door while walking out. so that was quite the adventure. i\\'m gonna go sleep now. goodnight! hey, i\\'m back again. my computer bsod\\'d while writing this, so i have to start this section over again. that\\'s why you save your work, kids! before i had to start over again, i was talking about languages. yes, i decided to bring that topic back after a while. but i no longer want to talk about it. why? because it\\'ll probably bore you to death. that is assuming you\\'re reading this at all. who knows, maybe absolutely zero people will read this within the span of the universe\\'s existence. but i doubt that. there\\'s gotta be someone who\\'ll find this text and dedicate their time to reading it, even if it takes thousands of years for that to happen. what will happen to this lte in a thousand years? will the entire internet dissapear within that time? in that case, will this text dissapear with it? or will it, along with the rest of what used to be the internet, be preserved somewhere? i\\'m thinking out loud right now. well, not really \"out loud\" because i\\'m typing this, and you can\\'t technically be loud through text. the closest thing is typing in all caps. imagine if i typed this entire text like that. that would be painful. i decided to actually save my work this time, in case of another crash. i already had my two past attempts at an lte vanish from existance. i mean, most of this lte is already stored on neocities, so i probably won\\'t need to worry about anything. i think i might change the lte page a little. i want the actual text area to be larger. i\\'m gonna make it a very basic html page with just a header and text. maybe with some css coloring. i don\\'t know. screw it, i\\'m gonna do it. there, now the text area is larger. it really does show how small this lte is so far compared to flamingchicken or hermnerps. but at least i made the background a nice alice blue. that\\'s the name of the css color i used. it\\'s pretty light. we\\'re getting pretty close to the 1/10 mark! that\\'s the point where we\\'re one tenth of the way to making this the longest text ever, meaning all i have to do is write the equivalent of everything i\\'ve already written so far nine more times! not gonna make any promises, though. how come every time i try to type \"though\", it comes out as \"thought\"? why do i always type the extra t? it\\'s so annoying that i have to delete the t every time. okay, only mildly annoying. not as annoying as i previously described. i apologize for my exaggeration of the annoyance level of me typing \"thought\" instead of \"though\". i just realized that most of the games i play are games that i\\'ve been playing for at least six years. i started playing garry\\'s mod in 2013, minecraft in whatever year version 1.2.3 came out. now i have to look that up. march 2, 2012. so i started playing minecraft approximately during that time. wow, seven years ago! coincidentally, i was also seven years old then. i remember the days of 2012-13. that was when i still played roblox and made terrible youtube videos. i was called \"infinite budgets\" back then. i also remember the days of 2016. a lot of people thought that was a terrible year, but for me personally, it brings me a lot of nostalgia because i talked a lot with my online friend at the time, and i did livestreams on youtube and stuff. it was fun. 2016 was also when i got the phone that i still have to this day. yup, my phone is three years old. my life was completely different when i got this phone: i was 11 years old, my youtube channel actually had activity, and i wasn’t writing this text. i’m currently writing this in the car. we are on out way to the dollar store. and since i’m writing this on my phone, i’m making a lot more typos than usual. some of them might make it through, so be prepared for that. anyways, we appear to be getting close to the dollar store. i have a gift card for that place. i think so anyways, it might be for a different store... yup, this dollar store is different. oh well. my sister has an obsession with sponges. i’m sure she’s gonna find the sponges and go crazy over them. why does she like sponges so much? no idea. she just found a bag of tiny baby dolls, and she wants to put them in ice cubes and call it “ice ice baby”. she is truly a strange human being. my sister also has an obsession with stuffies. she has such an addiction, that she’s banned from them. now she found the wigs and she’s considering buying one. she’s been looking at them for quite a while now. we’re out of the dollar store, and now we’re going to the computer store. i have no idea why we’re here. i guess we just are. now we’re going home. welp, that was a fun adventure. stay tuned for more fun adventures as you read through this lte. i should go now. bye! hello again. i made a private world on ourworldoftext for my sister and i, but she doesn\\'t want to join it. she doesn\\'t think it\\'ll be fun. now i\\'m just editing it alone. how sad. but oh well. now i’m here adding more to this text. i once made a discord server specifically for a language called “bo”, where the only word is “bo”. i made it almost four months ago, and somehow, it’s still going. people are still spamming nothing but “bo” there. it’s great. i also once made a server where you’re not allowed to use any vowels. it was a very strange server. i deleted it after some time though, so all that insanity is no more. i also used to own a pig latin server, but it got inactive so i deleted that too. we had some good memories in that server though. now there’s a new pig latin server, but it’s not owned by me. dang, my youtube channel has been dead for so long. i haven’t posted a video in a year. i want to revive it, but i don’t know what to post there. i’ll figure it out. i doubt my channel will ever go back to it’s 2016 legacy, but i’m sure i’ll post something eventually. random fact of the day: there are thirty-nine question marks so far in this text. am i about to make it forty? yes, i just did. now the fact i initially stated is no longer true. or is it? because i said “so far” in the fact, that implies that we’re talking about the moment that fact was said, disregarding any future events. now i’m pretty sure that fact is still technically true. welp, i guess i should just accept that i’m editing that world of text alone for the rest of my life. i originally put a bunch of complaining in there, but i deleted it all. the thing is, now that world will never be same without all of that complaining about my sister not being here. but that’s fine. hey, i just had a cool realization. basically, there’s this conlang (constructed language, for those not in the know) server where we have a sentence of the week activity. in this activity, someone posts a text with a maximum of nine sentences, then people translate it into their own conlangs. my realization is this: if we take nine sentences from this lte every week, there would be a whole year of sentences for people to translate. there are approximantly 523 sentences in this lte. divide that by 9 sentences each week, and you get 58 weeks worth of sentences, which is approximantly the number of weeks in a year. quick maths. i actually suck at math, but that’s besides the point. i should go now. goodbye! hello, i’m back again. i really need to come up with different hello and goodbye messages, because i’ve already said “hello, i’m back again” once before. same with the “i should go now. goodbye!” i said at the end of the previous section. i was going to explain what a “section” is, but i’m terrible at explaining things, so i’m not going to anymore. i guess you’ll just have to figure it out yourself. it’s probably not very hard to figure out, anyways. i guess i can just say that a section starts with me saying hello, and ends with me saying goodbye. that should be enough explaination, now that i think about it. hey, do you ever feel like you never have any idea what you’re talking about? that’s my entire life. i just summarized it all in one sentence. on an unrelated note, i feel like half this lte is just me talking about the lte itself. i mean, press ctrl+f on this webpage, then type “lte”. look at all the times i use it in this text! not counting the ‘lte’ in the word ‘multe’, of course. dang, now the search results will include that, too. anyways, half of this text is just me talking about how i’m trying to get this text to be the longest. well, the longest lte, anyways. i still have a long way to go. i’m only 12.7% of the way there. i mean, minus the four month gap, my estimation is that i’ve only been writing this for not even two weeks. so it makes sense that this lte isn’t very long yet. whenever i look at this webpage, it looks long at first glance, but the longer i look at it, the more i realize how short it actually is. it’s something that i can’t explain. for real this time. i just realized that none of this is helping the fact that half this lte is about the lte itself. i should bring up a new topic, but i don’t feel comfortable talking about much else. why? because, like i said, i never have any idea what i’m talking about. most of this lte is just me talking about ltes or languages. sometimes furries, but i don’t wanna go back into that territory at this point. but it doesn’t matter, because i’m still gonna write this lte for as long as possible, even if it means talking about the same things half the time. also, learn viesa! haven’t said that in a while, so i might as well bring it back. the documentation for viesa is on this very website, so go ahead and read it! you might need to know some linguistic knowledge to understand it, though. in fact, you probably won’t understand most of it unless you know some amount about lingusitics, so you have been warned. if viesa is too much for you, pig latin will probably be better for you. if it\\'s so easy that kids can learn it, you can too! it\\'s a language you can learn in probably five minutes, so why not give it a try? you may also enjoy ubbi dubbi, where you place \\'ub\\' before every vowel sound. it\\'s also a very easy language to learn, although not quite as popular. the thing is, none of these are even real languages. they\\'re just codes, and very simple codes at that. you could probably crask pig latin or ubbi dubbi rather easily. viesa too, actually. but i still enjoy them occasionally, even if'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15242/15242 [00:00<00:00, 1310408.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15242"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "maxlen = 100      #a kiválasztott stringek hossza\n",
    "step = 2           #létrehozunk egy új sentencet 2 lépésenként\n",
    "sentences = []     #tartalmazza a kiszedett sentenceket \n",
    "next_chars = []\n",
    "\n",
    "for i in tqdm(range(0, len(text) - maxlen, step)):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello, everyone! this is the longest text ever! i was inspired by the various other \"longest texts e'  :  v\n",
      "'llo, everyone! this is the longest text ever! i was inspired by the various other \"longest texts eve'  :  r\n",
      "'o, everyone! this is the longest text ever! i was inspired by the various other \"longest texts ever\"'  :   \n",
      "' everyone! this is the longest text ever! i was inspired by the various other \"longest texts ever\" o'  :  n\n",
      "'veryone! this is the longest text ever! i was inspired by the various other \"longest texts ever\" on '  :  t\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): print(repr(sentences[i]),\" : \", next_chars[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '%', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~', '‘', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))  #a szöveg különleges karaktereit tartalmazza\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '!': 1, '\"': 2, '%': 3, \"'\": 4, '(': 5, ')': 6, '+': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'a': 24, 'b': 25, 'c': 26, 'd': 27, 'e': 28, 'f': 29, 'g': 30, 'h': 31, 'i': 32, 'j': 33, 'k': 34, 'l': 35, 'm': 36, 'n': 37, 'o': 38, 'p': 39, 'q': 40, 'r': 41, 's': 42, 't': 43, 'u': 44, 'v': 45, 'w': 46, 'x': 47, 'y': 48, 'z': 49, '~': 50, '‘': 51, '’': 52, '“': 53, '”': 54}\n"
     ]
    }
   ],
   "source": [
    "char_indices = dict((char, chars.index(char)) for char in chars)  #mapba teszi el a karaktereket és azok indexeit\n",
    "print(char_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jenei\\AppData\\Local\\Temp\\ipykernel_17740\\293909962.py:1: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)        #a karaktereket bináris listává kódolja át\n",
      "C:\\Users\\jenei\\AppData\\Local\\Temp\\ipykernel_17740\\293909962.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
      "15242it [00:00, 40972.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (15242, 100, 55)\n",
      "y (15242, 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)        #a karaktereket bináris listává kódolja át\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in tqdm(enumerate(sentences)):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print(\"x\", x.shape)\n",
    "print(\"y\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_6 (GRU)                 (None, 16)                3504      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 55)                935       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,439\n",
      "Trainable params: 4,439\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf                  #build network\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.GRU(16, input_shape=(maxlen, len(chars))),\n",
    "    tf.keras.layers.Dense(units=len(chars), activation='softmax')\n",
    "]) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.01))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 5s 26ms/step - loss: 2.9238\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    model.fit(x, \n",
    "            y,\n",
    "            batch_size=128,\n",
    "            epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', and appearantly it\\'s \"i\", with 59 uses! the word \"i\" makes up 5% of the words this text! i would\\'v'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random             #kiválaszt egy random sort amivel kezdődni fog a generált szöveg\n",
    "\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "base_text = text[start_index: start_index + maxlen]\n",
    "base_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):     #betanul\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp:  0.2\n",
      ", and appearantly it's \"i\", with 59 uses! the word \"i\" makes up 5% of the words this text! i would'v\n",
      "e the the the the an is the the an at the the the the the the the the an at the the the the thar at  at the the the the the than the the the the the the the the the the the it the the the the the the th at the the it the an the te the the the the the it the the the bin the than the the that e the the the the the the in an the the the the at that the the the an i the te the the the te the ar i the \n",
      "Temp:  0.5\n",
      ", and appearantly it's \"i\", with 59 uses! the word \"i\" makes up 5% of the words this text! i would'v\n",
      "en thns a topt ins ad the ons mhe the t te the inthot tha at i an al arat ol f a ind than ot that bong itharan age t the than it t at o tug tre te te uid at thant at y arre t thant athe iol tbin than e thentin t aa ia i the the at bon oe mit ba man ave oo ateng it ton y tout a e t re lh om gont ira as y am t ad went the it nbe ton th ann ot the int il ath ot at thim the the ot onl e twe an mhe thi\n",
      "Temp:  1.0\n",
      ", and appearantly it's \"i\", with 59 uses! the word \"i\" makes up 5% of the words this text! i would'v\n",
      "dy   ake, r tlkmklceit, soug  thed biny y pme rham . dei aarto oh fine5d. k. finxthodld iteg0 hel anty ag bhonblaur i\"k bbe-gffohktte, cy ticy. tintenan thin jok s ilar metrir whos,m'sinxtenr ad tew t eivonge lren. le wa tis moig thbanll(oe bav5iu onul er rk ih atordtelntnnvon re  itin ghyadeceenti am ghe. 9hvpuathls in tgo? oro't fwe lholrrpirter ’ny baotant u i bc in al et oy eng atexte singte o\n",
      "Temp:  1.2\n",
      ", and appearantly it's \"i\", with 59 uses! the word \"i\" makes up 5% of the words this text! i would'v\n",
      "jsr sot cod bottt isinvokles why s renlcad as /"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [92], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t, char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(generated_text):\n\u001b[0;32m     12\u001b[0m     sampled[\u001b[38;5;241m0\u001b[39m, t, char_indices[char]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m          \n\u001b[1;32m---> 14\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]      \n\u001b[0;32m     16\u001b[0m next_index \u001b[38;5;241m=\u001b[39m sample(preds, temp)\n\u001b[0;32m     17\u001b[0m next_char \u001b[38;5;241m=\u001b[39m chars[next_index]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\as\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\as\\lib\\site-packages\\keras\\engine\\training.py:2249\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2247\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_begin()\n\u001b[0;32m   2248\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, iterator \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[0;32m   2250\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   2251\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\as\\lib\\site-packages\\keras\\engine\\data_adapter.py:1307\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1307\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[0;32m   1309\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\as\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:499\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    501\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\as\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:696\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    694\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 696\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\as\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:721\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(ds_variant):\n\u001b[0;32m    717\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    718\u001b[0m       gen_dataset_ops\u001b[38;5;241m.\u001b[39manonymous_iterator_v3(\n\u001b[0;32m    719\u001b[0m           output_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types,\n\u001b[0;32m    720\u001b[0m           output_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_shapes))\n\u001b[1;32m--> 721\u001b[0m   \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\as\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3408\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3407\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3408\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3409\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3411\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys                             #a lényeg, a szöveg legenerálása\n",
    "temperatures = [0.2, 0.5, 1.0, 1.2]\n",
    "gen_characters = 400\n",
    "\n",
    "for temp in temperatures:       #generálunk 400 karaktert különböző sebességgel\n",
    "    print(\"Temp: \", temp)\n",
    "    generated_text = base_text\n",
    "    print(generated_text)\n",
    "    for i in range(gen_characters):\n",
    "        sampled = np.zeros((1, maxlen, len(chars)))    \n",
    "        for t, char in enumerate(generated_text):\n",
    "            sampled[0, t, char_indices[char]] = 1.          \n",
    "        \n",
    "        preds = model.predict(sampled, verbose=0)[0]      \n",
    "        \n",
    "        next_index = sample(preds, temp)\n",
    "        next_char = chars[next_index]\n",
    "\n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
